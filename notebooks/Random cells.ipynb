{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cont = pd.read_csv('./data/s_contents.csv')\n",
    "s_cont.drop(columns=['cast'], inplace=True)\n",
    "s_cont.head(86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_contents['s_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shows['s_title']=shows['title'].map(lambda x: x.replace(' ','_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_info =[]\n",
    "\n",
    "url3='http://asianwiki.com/Thirty_But_Seventeen' #%shows['s_title'][21] \n",
    "res = requests.get(url3, headers = {'User-agent': 'SB 2.0'})\n",
    "soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "info = soup.find('div',{'id':'mw-content-text'})\n",
    "table = info.find_all('ul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 2: Using the show URL, scrape each show page and get user ratings\n",
    "\n",
    "k_titles = pd.read_csv('./data/k_titles.csv')\n",
    "\n",
    "show_url = k_titles['url'][5] + '#modal-review-see-all'\n",
    "\n",
    "# use selenium webdriver as library that acts as a \n",
    "# headless browser to access modal window\n",
    "driver = webdriver.Chrome(chrome_options=opts)\n",
    "driver.implicitly_wait(30)\n",
    "driver.get(show_url)\n",
    "test = []\n",
    "\n",
    "for i in range(3428):\n",
    "    time.sleep(3)\n",
    "    soup_level1=BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "    table = soup_level1.find_all('ul',{'class':'media-list'})\n",
    "    for row in table[2].find_all('li',{'class':'media'}):\n",
    "        review = {}\n",
    "        review['title'] = k_titles['title'][5]\n",
    "        review['user'] = row.find('a').attrs['href'][7:]\n",
    "        review['rating'] = row.find('span', {'class':'strong'}).text\n",
    "        test.append(review)\n",
    "            \n",
    "   \n",
    "\n",
    "\n",
    "review_6 = pd.DataFrame(test)\n",
    "print(review_6.shape[0])\n",
    "print(review_6['user'].nunique())\n",
    "# reviews_5 = pd.DataFrame(reviews5)\n",
    "# print(reviews_5.shape[0])\n",
    "# print(reviews_5['user'].nunique())\n",
    "\n",
    "# testp\n",
    "\n",
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>\n",
    "\n",
    "## scraped reviews\n",
    "|movie|reported|retrieved|%|\n",
    "|---|---|---|--|\n",
    "|review 2 | 7106| 2195| 30.9|\n",
    "|review 1 | 1354| 516| 38.1|\n",
    "|review 3 | 3683| 1545| 41.9|\n",
    "|review 4 | 4850| 1818| 37.5|\n",
    "|review 5 | 2123| 660| 31.1|\n",
    "|review 6 | 34282|||\n",
    "\n",
    "518/34282\n",
    "\n",
    "#end the Selenium browser session\n",
    "driver.quit()\n",
    "\n",
    "review_6.to_csv('./data/show6_reviews.csv')\n",
    "\n",
    "reviews = []\n",
    "for show in range(len(shows)):\n",
    "    rest_url = k_titles['url']\n",
    "    new_res = requests.get(rest_url)#modal-review-see-all\n",
    "    soup = BeautifulSoup(new_res.content, 'lxml')\n",
    "\n",
    "    user = soup.find_all('div', {'class':'media-body'})\n",
    "    for item in user:\n",
    "        review = {}\n",
    "        review['user'] = item.find('a').attrs['href'][17:]\n",
    "        show['rating'] = row.find('a').attrs['href'][12:].title()\n",
    "        reviews.append(review)\n",
    "# foods[100:150]\n",
    "\n",
    "### Step 4: Create a pandas DataFrame from your list of foods\n",
    "\n",
    "df = pd.DataFrame(reviews)\n",
    "df.head(10)\n",
    "\n",
    "### Step 5: Export to csv\n",
    "\n",
    "**Note:** Don't export the index column from your DataFrame\n",
    "\n",
    "filepaths = ['./data/shows_part_1.csv', './data/shows_part_2.csv', './data/shows_part_3.csv','./data/shows_part_4.csv','./data/shows_part_5.csv']\n",
    "df = pd.concat(map(pd.read_csv, filepaths))\n",
    "\n",
    "\n",
    "df\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.tail()\n",
    "\n",
    "df.to_csv('./data/pages_info.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url3='https://www.viki.com/tv/35734c-devilish-joy#modal-casts' \n",
    "res = requests.get(url3, headers= headers)\n",
    "soup = BeautifulSoup(res.content, 'lxml')\n",
    "actors = soup.find_all('div',{'itemprop':'actor'})\n",
    "actors[1].find('span', {'itemprop':'name'}).text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
